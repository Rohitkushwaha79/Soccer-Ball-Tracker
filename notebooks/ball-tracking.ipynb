{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8547996,"sourceType":"datasetVersion","datasetId":5107479},{"sourceId":8555525,"sourceType":"datasetVersion","datasetId":5112901},{"sourceId":8574230,"sourceType":"datasetVersion","datasetId":5127058}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Soccer Ball Tracking ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport math\nimport shutil\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image, ImageSequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uninstall aiohttp\n!pip uninstall -y aiohttp\n\n# Clean up residual files if any\ndir_path = '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info'\nif os.path.exists(dir_path):\n    shutil.rmtree(dir_path)\n\n# Reinstall aiohttp\n!pip install aiohttp\n\n# Verify the installation\nimport aiohttp\nprint(aiohttp.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n\n!pip install tensorflow[and-cuda]\n\n!pip install parse\nimport parse\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-31T10:17:23.279318Z","iopub.execute_input":"2024-05-31T10:17:23.279805Z","iopub.status.idle":"2024-05-31T10:19:42.326875Z","shell.execute_reply.started":"2024-05-31T10:17:23.279778Z","shell.execute_reply":"2024-05-31T10:19:42.325722Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu117\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: tensorflow[and-cuda] in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.59.3)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow[and-cuda])\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting nvidia-cublas-cu12==12.2.5.6 (from tensorflow[and-cuda])\n  Downloading nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.2.142 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.4.25 (from tensorflow[and-cuda])\n  Downloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.8.103 (from tensorflow[and-cuda])\n  Downloading nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.3.141 (from tensorflow[and-cuda])\n  Downloading nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.2.141 (from tensorflow[and-cuda])\n  Downloading nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.2.141 (from tensorflow[and-cuda])\n  Downloading nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.16.5 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting tensorrt==8.6.1.post1 (from tensorflow[and-cuda])\n  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting tensorrt-bindings==8.6.1 (from tensorflow[and-cuda])\n  Downloading tensorrt_bindings-8.6.1-cp310-none-manylinux_2_17_x86_64.whl.metadata (621 bytes)\nINFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.\nCollecting tensorflow[and-cuda]\n  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nCollecting ml-dtypes~=0.3.1 (from tensorflow[and-cuda])\n  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.31.0)\nCollecting tensorboard<2.17,>=2.16 (from tensorflow[and-cuda])\n  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.3)\nCollecting nvidia-cublas-cu12==12.3.4.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.7.29 (from tensorflow[and-cuda])\n  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.12.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.4.107 (from tensorflow[and-cuda])\n  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.4.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.2.0.103 (from tensorflow[and-cuda])\n  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.42.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow[and-cuda]) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow[and-cuda]) (0.1.2)\nDownloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (22.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl (98.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl (125.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl (197.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ml-dtypes, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tensorflow\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.16.1 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.16.1 which is incompatible.\ntf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ml-dtypes-0.3.2 nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-cupti-cu12-12.3.101 nvidia-cuda-nvcc-cu12-12.3.107 nvidia-cuda-nvrtc-cu12-12.3.107 nvidia-cuda-runtime-cu12-12.3.101 nvidia-cudnn-cu12-8.9.7.29 nvidia-cufft-cu12-11.0.12.1 nvidia-curand-cu12-10.3.4.107 nvidia-cusolver-cu12-11.5.4.101 nvidia-cusparse-cu12-12.2.0.103 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 tensorboard-2.16.2 tensorflow-2.16.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class ChannelAttentionModule(nn.Module):\n    def __init__(self, channel, ratio=16):\n        super(ChannelAttentionModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n \n        self.shared_MLP = nn.Sequential(\n            nn.Conv2d(channel, channel // ratio, 1, bias=False),\n            nn.ReLU(),\n            nn.Conv2d(channel // ratio, channel, 1, bias=False)\n        )\n        self.sigmoid = nn.Sigmoid()\n \n    def forward(self, x):\n        avgout = self.shared_MLP(self.avg_pool(x))\n        maxout = self.shared_MLP(self.max_pool(x))\n        return self.sigmoid(avgout + maxout)\n \nclass SpatialAttentionModule(nn.Module):\n    def __init__(self):\n        super(SpatialAttentionModule, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)\n        self.sigmoid = nn.Sigmoid()\n \n    def forward(self, x):\n        #map尺寸不变，缩减通道\n        avgout = torch.mean(x, dim=1, keepdim=True)\n        maxout, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avgout, maxout], dim=1)\n        out = self.sigmoid(self.conv2d(out))\n        return out\n \nclass CBAM(nn.Module):\n    def __init__(self, channel):\n        super(CBAM, self).__init__()\n        self.channel_attention = ChannelAttentionModule(channel)\n        self.spatial_attention = SpatialAttentionModule()\n \n    def forward(self, x):\n        out = self.channel_attention(x) * x\n        # out = self.spatial_attention(out) * out\n        return out\n\nclass Conv2DBlock(nn.Module):\n    \"\"\" Conv + ReLU + BN\"\"\"\n    def __init__(self, in_dim, out_dim, kernel_size, padding='same', bias=True, **kwargs):\n        super(Conv2DBlock, self).__init__(**kwargs)\n        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, bias=bias)\n        self.bn = nn.BatchNorm2d(out_dim)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\nclass Double2DConv(nn.Module):\n    \"\"\" Conv2DBlock x 2\"\"\"\n    def __init__(self, in_dim, out_dim):\n        super(Double2DConv, self).__init__()\n        self.conv_1 = Conv2DBlock(in_dim, out_dim, (3, 3))\n        self.conv_2 = Conv2DBlock(out_dim, out_dim, (3, 3))\n\n    def forward(self, x):\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        return x\n\nclass Double2DConv2(nn.Module):\n    \"\"\" Conv2DBlock x 2\"\"\"\n    def __init__(self, in_dim, out_dim):\n        super(Double2DConv2, self).__init__()\n        self.conv_1 = Conv2DBlock(in_dim, out_dim, (1, 1))\n        self.conv_2 = Conv2DBlock(out_dim, out_dim, (3, 3))\n\n        self.conv_3 = Conv2DBlock(in_dim, out_dim, (3, 3))\n        self.conv_4 = Conv2DBlock(out_dim, out_dim, (3, 3))\n\n        self.conv_5 = Conv2DBlock(in_dim, out_dim, (5, 5))\n        self.conv_6 = Conv2DBlock(out_dim, out_dim, (3, 3))\n\n        self.conv_7 = Conv2DBlock(out_dim*3, out_dim, (3, 3))\n\n    def forward(self, x):\n        x1 = self.conv_1(x)\n        x1 = self.conv_2(x1)\n\n        x2 = self.conv_3(x)\n        x2 = self.conv_4(x2)\n\n        x3 = self.conv_5(x)\n        x3 = self.conv_6(x3)\n\n        x = torch.cat([x1, x2, x3], dim=1)\n\n        x = self.conv_7(x)\n        x = x + x2\n\n        return x\n    \nclass Triple2DConv(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(Triple2DConv, self).__init__()\n        self.conv_1 = Conv2DBlock(in_dim, out_dim, (3, 3))\n        self.conv_2 = Conv2DBlock(out_dim, out_dim, (3, 3))\n        self.conv_3 = Conv2DBlock(out_dim, out_dim, (3, 3))\n\n    def forward(self, x):\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        x = self.conv_3(x)\n        return x\n\nclass TrackNetV2(nn.Module):\n    \"\"\" Original structure but less two layers \n        Total params: 10,161,411\n        Trainable params: 10,153,859\n        Non-trainable params: 7,552\n    \"\"\"\n    def __init__(self, in_dim=9, out_dim=3):\n        super(TrackNetV2, self).__init__()\n        self.down_block_1 = Double2DConv2(in_dim=in_dim, out_dim=64)\n        self.down_block_2 = Double2DConv2(in_dim=64, out_dim=128)\n        self.down_block_3 = Double2DConv2(in_dim=128, out_dim=256)\n        self.bottleneck = Triple2DConv(in_dim=256, out_dim=512)\n        self.up_block_1 = Double2DConv(in_dim=768, out_dim=256)\n        self.up_block_2 = Double2DConv(in_dim=384, out_dim=128)\n        self.up_block_3 = Double2DConv(in_dim=192, out_dim=64)\n        self.predictor = nn.Conv2d(64, out_dim, (1, 1))\n        self.sigmoid = nn.Sigmoid()\n        self.cbam1 = CBAM(channel=256) #only channel attention\n        self.cbam2 = CBAM(channel=128)\n        self.cbam3 = CBAM(channel=64)\n\n        self.cbam0_2 = CBAM(channel=256)\n        self.cbam1_2 = CBAM(channel=128)\n        self.cbam2_2 = CBAM(channel=64)\n\n    def forward(self, x):\n        \"\"\" model input shape: (F*3, 288, 512), output shape: (F, 288, 512) \"\"\"\n        x1 = self.down_block_1(x)                                   # (64, 288, 512)\n        x = nn.MaxPool2d((2, 2), stride=(2, 2))(x1)                 # (64, 144, 256)\n        x2 = self.down_block_2(x)                                   # (128, 144, 256)\n        x = nn.MaxPool2d((2, 2), stride=(2, 2))(x2)                 # (128, 72, 128)\n        x3 = self.down_block_3(x)                                   # (256, 72, 128), one less conv layer\n        x = nn.MaxPool2d((2, 2), stride=(2, 2))(x3)                 # (256, 36, 64)\n        x = self.bottleneck(x)                                      # (512, 36, 64)\n        x3 = self.cbam0_2(x3)\n        x = torch.cat([nn.Upsample(scale_factor=2)(x), x3], dim=1)  # (768, 72, 128) 256+512\n        \n        x = self.up_block_1(x)                                      # (256, 72, 128), one less conv layer\n        x = self.cbam1(x)\n        x2 = self.cbam1_2(x2)\n        x = torch.cat([nn.Upsample(scale_factor=2)(x), x2], dim=1)  # (384, 144, 256) 256+128\n        \n        x = self.up_block_2(x)                                      # (128, 144, 256)\n        x = self.cbam2(x)\n        x1 = self.cbam2_2(x1)\n        x = torch.cat([nn.Upsample(scale_factor=2)(x), x1], dim=1)  # (192, 288, 512) 128+64\n        \n        x = self.up_block_3(x)                                      # (64, 288, 512)\n        x = self.cbam3(x)\n        x = self.predictor(x)                                       # (3, 288, 512)\n        x = self.sigmoid(x)\n        return  x\n\n\n# from torchsummary import summary\n# Tr = TrackNetV2().cuda()\n# summary(Tr, (9, 288, 512))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:35:58.695375Z","iopub.execute_input":"2024-06-03T04:35:58.696354Z","iopub.status.idle":"2024-06-03T04:35:58.732443Z","shell.execute_reply.started":"2024-06-03T04:35:58.696321Z","shell.execute_reply":"2024-06-03T04:35:58.731572Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"HEIGHT = 288\nWIDTH = 512\ndata_dir = '/kaggle/input/soccerball-tracking/ball_tracking'\n\n\ndef list_dirs(directory):\n    \"\"\"Return a sorted list of directory paths including input directory.\"\"\"\n    return sorted([os.path.normpath(os.path.join(directory, path)).replace(\"\\\\\", \"/\") for path in os.listdir(directory)])\n\ndef get_model(model_name, num_frame, input_type):\n    \"\"\" Create model by name and the configuration parameter.\n\n        args:\n            model_name - A str of model name\n            num_frame - An int specifying the length of a single input sequence\n            input_type - A str specifying input type\n                '2d' for stacking all the frames at RGB channel dimesion result in shape (H, W, F*3)\n                '3d' for stacking all the frames at extra dimesion result in shape (F, H, W, 3)\n\n        returns:\n            model - A keras.Model\n            input_shape - A tuple specifying the input shape (for model.summary)\n    \"\"\"\n    # Import model\n    \n    if model_name in ['TrackNetV2']:\n        model = TrackNetV2(in_dim=num_frame*3, out_dim=num_frame)\n    \n    return model\n\ndef model_summary(model, model_name):\n    total_count = 0\n    total_byte_coubt = 0\n    for param in model.parameters():\n        total_count += param.nelement()\n        total_byte_coubt += param.nelement()*param.element_size()\n    print('=======================================')\n    print(f'Model: {model_name}')\n    print(f'Number of parameters: {total_count}.')\n    print(f'Memory usage of : {total_byte_coubt/1024/1024:.4f} MB')\n    print('=======================================')\n\ndef frame_first_RGB(input, input_type):\n    \"\"\" Helper function for transforming x to cv image format.\n\n        args:\n            input - A numpy.ndarray of RGB image sequences with shape (N, input_shape)\n            input_type - A str specifying input type\n                '2d' for stacking all the frames at RGB channel dimesion result in shape (H, W, F*3)\n                '3d' for stacking all the frames at extra dimesion result in shape (F, H, W, 3)\n\n        returns:\n            A numpy.ndarray of RGB image sequences with shape (N, F, H, W, 3)\n    \"\"\"\n    assert len(input.shape) > 3\n    if input_type == '2d': # (N, F*3, H ,W)\n        input = np.transpose(input, (0, 2, 3, 1)) # (N, H ,W, F*3)\n    else: # (N, 3, F, H ,W)\n        return np.transpose(input, (0, 2, 3, 4, 1))\n    \n    # Case of input_type == '2d'\n    num_frame = int(input.shape[-1]/3)\n    tmp_img = np.array([]).reshape(0, num_frame, HEIGHT, WIDTH, 3)\n    for n in range(input.shape[0]):\n        tmp_frame = np.array([]).reshape(0, HEIGHT, WIDTH, 3)\n        for f in range(0, input.shape[-1], 3):\n            img = input[n, :, :, f:f+3]\n            tmp_frame = np.concatenate((tmp_frame, img.reshape(1, HEIGHT, WIDTH, 3)), axis=0)\n        tmp_img = np.concatenate((tmp_img, tmp_frame.reshape(1, num_frame, HEIGHT, WIDTH, 3)), axis=0)\n    \n    return tmp_img\n\ndef frame_first_RGBD(input, input_type):\n    \"\"\" Helper function for transforming x to cv image format.\n\n        args:\n            input - A numpy.ndarray of RGBD image sequences with shape (N, input_shape)\n            input_type - A str specifying input type\n                '2d' for stacking all the frames at RGB channel dimesion result in shape (H, W, F*3)\n                '3d' for stacking all the frames at extra dimesion result in shape (F, H, W, 3)\n\n        returns:\n            A numpy.ndarray of RGB image sequences with shape (N, F, H, W, 3)\n    \"\"\"\n    assert len(input.shape) > 3\n    if input_type == '2d': \n        # (N, F*4, H ,W)\n        input = np.transpose(input, (0, 2, 3, 1)) # (N, H ,W, F*4)\n    else: \n        # (N, 4, F, H ,W)\n        input = input[:, :-1, :, :, :]\n        return np.transpose(input, (0, 2, 3, 4, 1))\n    \n    # Case of input_type == '2d'\n    num_frame = int(input.shape[-1]/4)\n    tmp_img = np.array([]).reshape(0, num_frame, HEIGHT, WIDTH, 3)\n    for n in range(input.shape[0]):\n        tmp_frame = np.array([]).reshape(0, HEIGHT, WIDTH, 3)\n        for f in range(0, input.shape[-1], 4):\n            img = input[n, :, :, f:f+3]\n            tmp_frame = np.concatenate((tmp_frame, img.reshape(1, HEIGHT, WIDTH, 3)), axis=0)\n        tmp_img = np.concatenate((tmp_img, tmp_frame.reshape(1, num_frame, HEIGHT, WIDTH, 3)), axis=0)\n    \n    return tmp_img\n\ndef frame_first_Gray(input, input_type):\n    \"\"\" Helper function for transforming y to cv image format.\n\n        args:\n            input - A numpy.ndarray of gray scale image sequences with shape (N, input_shape)\n            input_type - A str specifying input type\n                '2d' for stacking all the frames at RGB channel dimesion result in shape (H, W, F*3)\n                '3d' for stacking all the frames at extra dimesion result in shape (F, H, W, 3)\n        returns:\n            img - A numpy.ndarray of scale imag sequences with shape (N, F, H, W)\n    \"\"\"\n    assert len(input.shape) > 3\n    if input_type == '2d':\n        # (N, F, H ,W)\n        return input\n    else: \n        # (N, 1, F, H ,W)\n        return np.squeeze(input, axis=1)\n\ndef get_num_frames(video_file):\n    \"\"\" Return the number of frames in the video.\n\n        args:\n            video_file - A str of video file path with format '{data_dir}/{split}/match{match_id}/video/{rally_id}.mp4\n\n        returns:\n            A int specifying the number of frames in the video\n    \"\"\"\n    # video_file: \n    assert video_file[-4:] == '.mp4'\n    print(video_file)\n    match_dir, rally_id = parse.parse('{}/video/{}.mp4', video_file) \n    frame_dir = f'{match_dir}/frame/{rally_id}'\n    assert os.path.exists(frame_dir)\n\n    return len(os.listdir(frame_dir))\n\ndef generate_frames(video_file):\n    \"\"\" Sample frames from the video.\n\n        args:\n            video_file - A str of video file path with format '{data_dir}/{split}/match{match_id}/video/{rally_id}.mp4\n    \"\"\"\n    try:\n        assert video_file[-4:] == '.mp4'\n        match_dir, rally_id = parse.parse('{}/video/{}.mp4', video_file)\n        csv_file = f'{match_dir}/csv/{rally_id}_ball.csv'\n        assert os.path.exists(video_file) and os.path.exists(csv_file)\n    except:\n        print(f'{video_file} no match csv file.')\n        return\n\n    frame_dir = f'{match_dir}/frame/{rally_id}'\n    if not os.path.exists(frame_dir):\n        # Haven't process\n        os.makedirs(frame_dir)\n    else:\n        label_df = pd.read_csv(csv_file, encoding='utf8')\n        if len(list_dirs(frame_dir)) != len(label_df):\n            # Some error occur\n            shutil.rmtree(frame_dir)\n            os.makedirs(frame_dir)\n        else:\n            # Already processed.\n            return\n\n    label_df = pd.read_csv(csv_file, encoding='utf8')\n    cap = cv2.VideoCapture(video_file)\n    num_frames = 0\n    success = True\n\n    # Sample frames until video end or exceed the number of labels\n    while success and num_frames != len(label_df):\n        success, image = cap.read()\n        if success:\n            cv2.imwrite(f'{frame_dir}/{num_frames}.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 95])\n\n            num_frames += 1\n\ndef get_eval_frame_pathes(tuple_array, data):\n    \"\"\" Get frame pathes according to the evaluation tuple results.\n\n        args:\n            tuple_array - A numpy.ndarray of the evaluation tuple results\n                each tuple specifying (sequence_id, frame_no)\n            data - A dictionary which stored the information for building dataset\n                data['filename']: A numpy.ndarray of frame pathe sequences with shape (N, F)\n                data['coordinates']: A numpy.ndarray of coordinate sequences with shape (N, F, 2)\n                data['visibility']: A numpy.ndarray of visibility sequences with shape (N, F) - \n\n        returns:\n            A list of frame pathes\n    \"\"\"\n    path_list = []\n    for (i, f) in tqdm(tuple_array):\n        path_list.append(data['filename'][i][f])\n    return sorted(path_list)\n\ndef get_eval_statistic(data_dir, path_list):\n    \"\"\" Count the number of frame pathes from each rally.\n\n        args:\n            data_dir - A str of the root directory of the dataset\n            path_list - A list of frame pathes\n\n        returns:\n            A dictionary specipying the statistic\n                each pair specifying {'{match_id}_{rally_id}': path_count}\n    \"\"\"\n    res_dict = {}\n    format_string = data_dir + '/{}/match{}/frame/{}/{}.jpg'\n    for path in tqdm(path_list):\n        _, m_id, c_id, _ = parse.parse(format_string, path)\n        key = f'{m_id}_{c_id}'\n        if key not in res_dict.keys():\n            res_dict[key] = 1\n        else:\n            res_dict[key] += 1\n    res_dict = sorted(res_dict.items(), key=lambda x:x[1], reverse=True)\n    return {k: c for k, c in res_dict}\n\n##################################  Training Functions ##################################\ndef WeightedBinaryCrossEntropy(y, y_pred):\n    # epsilon = 1e-7\n    loss = (-1)*(torch.square(1 - y_pred) * y * torch.log(torch.clamp(y_pred, 1e-7, 1)) + torch.square(y_pred) * (1 - y) * torch.log(torch.clamp(1 - y_pred, 1e-7, 1)))\n    return torch.mean(loss) # (N, 3, 288, 512)\n\ndef FocalWBCE(y, y_pred):\n    # epsilon = 1e-7\n    gamma = 1\n    loss = (-1)*(torch.square(1 - y_pred) * (torch.clamp(1 - y_pred, 1e-7, 1)** gamma) * y * torch.log(torch.clamp(y_pred, 1e-7, 1)) + torch.square(y_pred)* ((torch.clamp(y_pred, 1e-7, 1)) ** gamma) * (1 - y) * torch.log(torch.clamp(1 - y_pred, 1e-7, 1)))\n    return torch.mean(loss) # (N, 3, 288, 512)\n\ndef train(epoch, model, optimizer, loss_fn, data_loader, input_type, display_step, save_dir):\n    model.train()\n    data_prob = tqdm(data_loader)\n    epoch_loss = []\n    for step, (i, x, y, c) in enumerate(data_prob):\n        x, y = x.float().cuda(), y.float().cuda()\n        optimizer.zero_grad()\n        y_pred = model(x)\n        loss = loss_fn(y, y_pred)\n        epoch_loss.append(loss.item())\n        loss.backward()\n        optimizer.step()\n\n        if (step + 1) % display_step == 0:\n            show_prediction(x, y, y_pred, c, input_type, save_dir)\n            data_prob.set_description(f'Epoch [{epoch}]')\n            data_prob.set_postfix(loss=loss.item())\n    return float(np.mean(epoch_loss))\n\ndef evaluation(model, data_loader, tolerance, input_type):\n    model.eval()\n    data_prob = tqdm(data_loader)\n    TP, TN, FP1, FP2, FN = [], [], [], [], []\n    for step, (i, x, y, c) in enumerate(data_prob):\n        x, y = x.float().cuda(), y.float().cuda()\n        with torch.no_grad():\n            y_pred = model(x)\n        y_pred = y_pred > 0.5\n        # y_pred = y_pred > 0.4\n        tp, tn, fp1, fp2, fn = get_confusion_matrix(i, y_pred, y, c, tolerance, input_type=input_type)\n        TP.extend(tp)\n        TN.extend(tn)\n        FP1.extend(fp1)\n        FP2.extend(fp2)\n        FN.extend(fn)\n        \n        data_prob.set_description(f'Evaluation')\n        data_prob.set_postfix(TP=len(TP), TN=len(TN), FP1=len(FP1), FP2=len(FP2), FN=len(FN))\n    \n    accuracy, precision, recall = get_metric(len(TP), len(TN), len(FP1), len(FP2), len(FN))\n    print(f'\\nacc: {accuracy:.4f}\\tprecision: {precision:.4f}\\trecall: {recall:.4f}\\tTP: {len(TP)}\\tTN: {len(TN)}\\tFP1: {len(FP1)}\\tFP2: {len(FP2)}\\tFN: {len(FN)}')\n    return accuracy, precision, recall, TP, TN, FP1, FP2, FN\n\ndef get_confusion_matrix(indices, y_pred, y_true, y_coor, tolerance, input_type='3d'):\n    \"\"\" Helper function Generate input sequences from frames.\n\n        args:\n            indices - A tf.EagerTensor of indices for sequences\n            y_pred - A tf.EagerTensor of predicted heatmap sequences\n            y_true - A tf.EagerTensor of ground-truth heatmap sequences\n            y_coor - A tf.EagerTensor of ground-truth coordinate sequences\n            tolerance - A int speicfying the tolerance for FP1\n            input_type - A str specifying input type\n                '2d' for stacking all the frames at RGB channel dimesion result in shape (H, W, F*3)\n                '3d' for stacking all the frames at extra dimesion result in shape (F, H, W, 3)\n        returns:\n            TP, TN, FP1, FP2, FN - Lists of tuples of all the prediction results\n                                    each tuple specifying (sequence_id, frame_no)\n    \"\"\"\n    TP, TN, FP1, FP2, FN = [], [], [], [], []\n    y_pred, y_true = y_pred.detach().cpu().numpy(), y_true.detach().cpu().numpy()\n    y_pred = frame_first_Gray(y_pred, input_type)\n    y_true = frame_first_Gray(y_true, input_type)\n    for n in range(y_pred.shape[0]):\n        num_frame = y_pred.shape[1]\n        for f in range(num_frame):\n            y_p = y_pred[n][f]\n            y_t = y_true[n][f]\n            c_t = y_coor[n][f]\n            if np.amax(y_p) == 0 and np.amax(y_t) == 0:\n                # True Negative: prediction is no ball, and ground truth is no ball\n                TN.append((int(indices[n]), int(f)))\n            elif np.amax(y_p) > 0 and np.amax(y_t) == 0:\n                # False Positive 2: prediction is ball existing, but ground truth is no ball\n                FP2.append((int(indices[n]), int(f)))\n            elif np.amax(y_p) == 0 and np.amax(y_t) > 0:\n                # False Negative: prediction is no ball, but ground truth is ball existing\n                FN.append((int(indices[n]), int(f)))\n            elif np.amax(y_p) > 0 and np.amax(y_t) > 0:\n                # both prediction and ground truth are ball existing\n                h_pred = y_p * 255\n                h_true = y_t * 255\n                h_pred = h_pred.astype('uint8')\n                h_true = h_true.astype('uint8')\n                #h_pred\n                (cnts, _) = cv2.findContours(h_pred.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n                rects = [cv2.boundingRect(ctr) for ctr in cnts]\n                max_area_idx = 0\n                max_area = rects[max_area_idx][2] * rects[max_area_idx][3]\n                for i in range(len(rects)):\n                    area = rects[i][2] * rects[i][3]\n                    if area > max_area:\n                        max_area_idx = i\n                        max_area = area\n                target = rects[max_area_idx]\n                cx_pred, cy_pred = int(target[0] + target[2] / 2), int(target[1] + target[3] / 2)\n                cx_true, cy_true = int(c_t[0]), int(c_t[1])\n                dist = math.sqrt(pow(cx_pred-cx_true, 2)+pow(cy_pred-cy_true, 2))\n                if dist > tolerance:\n                    # False Positive 1: prediction is ball existing, but is too far from ground truth\n                    FP1.append((int(indices[n]), int(f)))\n                else:\n                    # True Positive\n                    TP.append((int(indices[n]), int(f)))\n    return TP, TN, FP1, FP2, FN\n\ndef get_metric(TP, TN, FP1, FP2, FN):\n    \"\"\" Helper function Generate input sequences from frames.\n\n        args:\n            TP, TN, FP1, FP2, FN - Each float specifying the count for each result type of prediction\n\n        returns:\n            accuracy, precision, recall - Each float specifying the value of metric\n    \"\"\"\n    try:\n        accuracy = (TP + TN) / (TP + TN + FP1 + FP2 + FN)\n    except:\n        accuracy = 0\n    try:\n        precision = TP / (TP + FP1 + FP2)\n    except:\n        precision = 0\n    try:\n        recall = TP / (TP + FN)\n    except:\n        recall = 0\n    return accuracy, precision, recall\n\n##################################  Prediction Functions ##################################\ndef get_frame_unit(frame_list, num_frame):\n    \"\"\" Sample frames from the video.\n\n        args:\n            frame_list - A str of video file path with format '{data_dir}/{split}/match{match_id}/video/{rally_id}.mp4\n\n        return:\n            frames - A tf.Tensor of a mini batch input sequence\n    \"\"\"\n    batch = []\n    # Get the resize scaler\n    h, w, _ = frame_list[0].shape\n    h_ratio = h / HEIGHT\n    w_ratio = w / WIDTH\n    \n    def get_unit(frame_list):\n        \"\"\" Generate an input sequence from frame pathes and labels.\n\n            args:\n                frame_list - A numpy.ndarray of single frame sequence with shape (F,)\n\n            returns:\n                frames - A numpy.ndarray of resized frames with shape (H, W, 3*F)\n        \"\"\"\n        frames = np.array([]).reshape(0, HEIGHT, WIDTH)\n\n        # Process each frame in the sequence\n        for img in frame_list:\n            img = cv2.resize(img, (WIDTH, HEIGHT))\n            img = np.moveaxis(img, -1, 0)\n            frames = np.concatenate((frames, img), axis=0)\n        \n        return frames\n    \n    # Form a mini batch of input sequence\n    for i in range(0, len(frame_list), num_frame):\n        frames = get_unit(frame_list[i: i+num_frame])\n        frames /= 255.\n        batch.append(frames)\n\n    batch = np.array(batch)\n    return torch.FloatTensor(batch)\n\ndef get_object_center(heatmap):\n    \"\"\" Get coordinates from the heatmap.\n\n        args:\n            heatmap - A numpy.ndarray of a single heatmap with shape (H, W)\n\n        returns:\n            ints specifying center coordinates of object\n    \"\"\"\n    if np.amax(heatmap) == 0:\n        # No respond in heatmap\n        return 0, 0\n    else:\n        # Find all respond area in the heapmap\n        (cnts, _) = cv2.findContours(heatmap.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        rects = [cv2.boundingRect(ctr) for ctr in cnts]\n\n        # Find largest area amoung all contours\n        max_area_idx = 0\n        max_area = rects[max_area_idx][2] * rects[max_area_idx][3]\n        for i in range(len(rects)):\n            area = rects[i][2] * rects[i][3]\n            if area > max_area:\n                max_area_idx = i\n                max_area = area\n        target = rects[max_area_idx]\n    \n    return int((target[0] + target[2] / 2)), int((target[1] + target[3] / 2))\n\ndef get_pred_type(cx_pred, cy_pred, cx, cy, tolerance):\n    \"\"\" Get the result type of the prediction.\n\n        args:\n            cx_pred, cy_pred - ints specifying the predicted coordinates\n            cx, cy - ints specifying the ground-truth coordinates\n            tolerance - A int speicfying the tolerance for FP1\n\n        returns:\n            A str specifying the result type of the prediction\n    \"\"\"\n    pred_has_ball = False if (cx_pred == 0 and cy_pred == 0) else True\n    gt_has_ball = False if (cx == 0 and cy == 0) else True\n    if  not pred_has_ball and not gt_has_ball:\n        return 'TN'\n    elif pred_has_ball and not gt_has_ball:\n        return 'FP2'\n    elif not pred_has_ball and gt_has_ball:\n        return 'FN'\n    else:\n        dist = math.sqrt(pow(cx_pred-cx, 2)+pow(cy_pred-cy, 2))\n        if dist > tolerance:\n            return 'FP1'\n        else:\n            return 'TP'\n\n################################  Visualization Functions ################################\n\ndef plot_result(loss_list=None, train_acc_dict=None, test_acc_dict=None, num_frame=3, save_dir='', model_name=''):\n    \"\"\" Plot training performance.\n\n        args:\n            loss_list - A list of epoch losses\n            train_acc_dict - A dictionary which stored statistic of evaluation on training set\n                structure {'TP':[], 'TN': [], 'FP1': [], 'FP2': [], 'FN': [], 'accuracy': [], 'precision': [], 'recall': []}\n            test_acc_dict - A dictionary which stored statistic of evaluation on testing set\n                structure {'TP':[], 'TN': [], 'FP1': [], 'FP2': [], 'FN': [], 'accuracy': [], 'precision': [], 'recall': []}\n            num_frame - An int specifying the length of a single input sequence\n            save_dir - A str specifying the save directory\n            model_name - A str of model name\n    \"\"\"\n    # Plot training epoch losses\n    if loss_list:\n        plt.title(f'{model_name} (f = {num_frame})\\nTraining Loss (WBCE)')\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.plot(loss_list)\n        plt.tight_layout()\n        plt.savefig(f'{save_dir}/loss.jpg')\n        plt.clf()\n\n    # Plot accuracy, precision, recall result from evaluation\n    plt.title(f'{model_name} (f = {num_frame})\\nPerformance')\n    if test_acc_dict:\n        # test_acc, test_precision, test_recall = np.max(test_acc_dict['accuracy']), np.max(test_acc_dict['precision']), np.max(test_acc_dict['recall'])\n        test_acc = np.max(test_acc_dict['accuracy']) #新增的\n        index_of_test = np.where(test_acc_dict['accuracy'] == test_acc)[0][0]\n        test_precision = test_acc_dict['precision'][index_of_test]\n        test_recall = test_acc_dict['recall'][index_of_test]\n\n\n        plt.plot(test_acc_dict['accuracy'], label='test_accuracy')\n        plt.plot(test_acc_dict['precision'], label='test_precision')\n        plt.plot(test_acc_dict['recall'], label='test_recall')\n    if train_acc_dict:\n        # train_acc, train_precision, train_recall = np.max(train_acc_dict['accuracy']), np.max(train_acc_dict['precision']), np.max(train_acc_dict['recall'])\n        train_acc = np.max(train_acc_dict['accuracy'])\n        index_of_train = np.where(train_acc_dict['accuracy'] == train_acc)[0][0]\n        train_precision = train_acc_dict['precision'][index_of_train]\n        train_recall = train_acc_dict['recall'][index_of_train]\n        \n        plt.plot(train_acc_dict['accuracy'], label='train_accuracy')\n        plt.plot(train_acc_dict['precision'], label='train_precision')\n        plt.plot(train_acc_dict['recall'], label='train_recall')\n        \n    if train_acc_dict and test_acc_dict:\n        plt.xlabel(f'epoch\\ntrain  accuracy: {train_acc*100.:.2f} %  precision: {train_precision*100.:.2f} %  recall: {train_recall*100.:.2f} %\\n test  accuracy: {test_acc*100.:.2f} %  precision: {test_precision*100.:.2f} %  recall: {test_recall*100.:.2f} %')\n    elif test_acc_dict:\n        plt.xlabel(f'epochn\\n test  accuracy: {test_acc*100.:.2f} %  precision: {test_precision*100.:.2f} %  recall: {test_recall*100.:.2f} %')\n    elif train_acc_dict:\n        plt.xlabel(f'epochn\\n test  accuracy: {train_acc*100.:.2f} %  precision: {train_precision*100.:.2f} %  recall: {train_recall*100.:.2f} %')\n    else:\n        pass\n    plt.ylabel('metric')\n    plt.ylim((0.,1.))\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/performance.jpg')\n    plt.close()\n\ndef plot_eval_statistic(FN_res, FP1_res, FP2_res, split, save_file, figsize=(12, 5)):\n    \"\"\" Plot the distribution of FN, FP1,and FP2 in all rallies.\n\n        args:\n            FN_res, FP1_res, FP2_res - Dictionaries which stored the statistic of each prediction result type\n                each pair specifying {'{match_id}_{rally_id}': path_count}\n            split - A str specify the split of dataset\n            save_file - A str specifying the save file name\n            figsize - A tuple specifying the size of figure with shape (W, H)\n    \"\"\"\n    rally_key = sorted(FN_res.keys())\n    FN_list, FP1_list, FP2_list = [], [], []\n    # Ensure every rally has value\n    for k in rally_key:\n        if k in FN_res.keys():\n            FN_list.append(FN_res[k])\n        else:\n            FN_list.append(0)\n        if k in FP1_res.keys():\n            FP1_list.append(FP1_res[k])\n        else:\n            FP1_list.append(0)\n        if k in FP2_res.keys():\n            FP2_list.append(FP2_res[k])\n        else:\n            FP2_list.append(0)\n    \n    # Plot stack bar chart\n    width = 0.8\n    x_tick = np.arange(len(rally_key))\n    FN_list, FP1_list, FP2_list = np.array(FN_list), np.array(FP1_list), np.array(FP2_list)\n    total_count = FN_list+FP1_list+FP2_list\n    plt.figure(figsize=figsize)\n    plt.title(f'{split} Set Error Analysis')\n    plt.xlabel('clip label')\n    plt.ylabel('frame count')\n    plt.ylim((0.,np.max(total_count)+60))\n    plt.bar(x_tick, FN_list, color='b', label='FN', width=width)\n    plt.bar(x_tick, FP1_list, bottom=FN_list, color='g', label='FP1', width=width)\n    plt.bar(x_tick, FP2_list, bottom=FN_list+FP1_list, color='r', label='FP2', width=width)\n    plt.xticks(x_tick, rally_key, rotation=90)\n    for i, c in zip(x_tick, total_count):\n        plt.text(x=i-width , y=c+10 , s=c, fontsize=12)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(f'{save_file}.jpg')\n    plt.close()\n\ndef show_prediction(x, y, y_pred, y_coor, input_type, save_dir):\n    \"\"\" Visualize the inupt sequence with its predicted heatmap.\n        Save as a gif image.\n\n        args:\n            x - A tf.EagerTensor of input sequences\n            y - A tf.EagerTensor of ground-truth heatmap sequences\n            y_pred - A tf.EagerTensor of predicted heatmap sequences\n            y_coor - A tf.EagerTensor of ground-truth coordinate sequences\n            input_type - A str specifying input type\n                '2d' for stacking all the frames at RGB channel dimesion result in shape (H, W, F*3)\n                '3d' for stacking all the frames at extra dimesion result in shape (F, H, W, 3)\n            save_dir - A str specifying the save directory\n    \"\"\"\n    imgs = []\n    x, y, y_pred, y_coor = x.detach().cpu().numpy(), y.detach().cpu().numpy(), y_pred.detach().cpu().numpy(), y_coor.detach().cpu().numpy()\n\n    # Transform to cv image format (N, F, H , W, C)\n    x = frame_first_RGB(x, input_type)\n    y = frame_first_Gray(y, input_type)\n    y_pred = frame_first_Gray(y_pred, input_type)\n\n    # Only plot the first sequence in the mini-batch\n    x, y, y_pred, y_coor = x[0], y[0], y_pred[0], y_coor[0]\n    y_map = y_pred > 0.5\n\n    # Scale value from [0, 1] to [0, 255]\n    x = x * 255\n    y = y * 255\n    y_p = y_pred * 255\n    y_m = y_map * 255\n    x = x.astype('uint8')\n    y = y.astype('uint8')\n    y_p = y_p.astype('uint8')\n    y_m = y_m.astype('uint8')\n    \n    # Write image sequence to gif\n    for f in range(y_coor.shape[0]):\n        # Stack channels to form RGB images\n        tmp_y = cv2.cvtColor(y[f], cv2.COLOR_GRAY2BGR)\n        tmp_pred = cv2.cvtColor(y_p[f], cv2.COLOR_GRAY2BGR)\n        tmp_map = cv2.cvtColor(y_m[f], cv2.COLOR_GRAY2BGR)\n        tmp_x = x[f]\n        assert tmp_x.shape == tmp_y.shape == tmp_pred.shape == tmp_map.shape\n\n        # Mark ground-truth label\n        if int(y_coor[f][0]) > 0 and int(y_coor[f][1]) > 0:\n            cv2.circle(tmp_x, (int(y_coor[f][0]), int(y_coor[f][1])), 2, (255, 0, 0), -1)\n        up_img = cv2.hconcat([tmp_x, tmp_y])\n        down_img = cv2.hconcat([tmp_pred, tmp_map])\n        img = cv2.vconcat([up_img, down_img])\n\n        # Cast cv image to PIL image for saving gif format\n        img = Image.fromarray(img)\n        imgs.append(img)\n        imgs[0].save(f'{save_dir}/pred_cur.gif', format='GIF', save_all=True, append_images=imgs[1:], duration=1000, loop=0)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:35:58.734218Z","iopub.execute_input":"2024-06-03T04:35:58.734546Z","iopub.status.idle":"2024-06-03T04:35:58.838806Z","shell.execute_reply.started":"2024-06-03T04:35:58.734515Z","shell.execute_reply":"2024-06-03T04:35:58.838051Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class Badminton_Dataset(Dataset):\n    def __init__(self, root_dir=data_dir, split='train', mode='2d', num_frame=3, slideing_step=1, frame_dir=None, debug=False):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.HEIGHT = 288\n        self.WIDTH = 512\n\n        self.mag = 1\n        self.sigma = 8\n\n        self.root_dir = root_dir\n        self.split = split\n        self.mode = mode\n        self.num_frame = num_frame\n        self.slideing_step = slideing_step\n\n        if not os.path.exists(os.path.join(self.root_dir, f'f{self.num_frame}_s{self.slideing_step}_{self.split}.npz')):\n            self._gen_frame_files()\n        data_dict = np.load(os.path.join(\"/kaggle/working/\", f'f{self.num_frame}_s{self.slideing_step}_{self.split}.npz'))\n        \n        if debug:\n            num_debug = 256\n            self.frame_files = data_dict['filename'][:num_debug] # (N, 3)\n            self.coordinates = data_dict['coordinates'][:num_debug] # (N, 3, 2)\n            self.visibility = data_dict['visibility'][:num_debug] # (N, 3)\n        elif frame_dir:\n            self.frame_files, self.coordinates, self.visibility = self._gen_frame_unit(frame_dir)\n        else:\n            self.frame_files = data_dict['filename'] # (N, 3)\n            self.coordinates = data_dict['coordinates'] # (N, 3, 2)\n            self.visibility = data_dict['visibility'] # (N, 3)\n\n    def _get_rally_dirs(self):\n        match_dirs = list_dirs(os.path.join(self.root_dir, self.split))\n        match_dirs = sorted(match_dirs, key=lambda s: int(s.split('match')[-1]))\n        rally_dirs = []\n        for match_dir in match_dirs:\n            rally_dir = list_dirs(os.path.join(match_dir, 'frame'))\n            rally_dirs.extend(rally_dir)\n\n        # print(rally_dirs)\n        return rally_dirs\n\n    def _gen_frame_files(self):\n        rally_dirs = self._get_rally_dirs()\n        frame_files = np.array([]).reshape(0, self.num_frame)\n        coordinates = np.array([], dtype=np.float32).reshape(0, self.num_frame, 2)\n        visibility = np.array([], dtype=np.float32).reshape(0, self.num_frame)\n        # print(rally_dirs)\n        # Generate input sequences from each rally\n        for rally_dir in tqdm(rally_dirs):\n            \n            match_dir, rally_id = parse.parse('{}/frame/{}', rally_dir)\n            \n            csv_file = os.path.join(match_dir, 'csv', f'{rally_id}_ball.csv')\n            try:\n                label_df = pd.read_csv(csv_file, encoding='utf8').sort_values(by='Frame').fillna(0)\n            except:\n                print(f'Label file {rally_id}_ball.csv not found.')\n                continue\n            \n            frame_file = np.array([os.path.join(rally_dir, f'{f_id}.jpg') for f_id in label_df['Frame']])\n            x, y, vis = np.array(label_df['X']), np.array(label_df['Y']), np.array(label_df['Visibility'])\n            assert len(frame_file) == len(x) == len(y) == len(vis)\n\n            # Sliding on the frame sequence\n            for i in range(0, len(frame_file)-self.num_frame, self.slideing_step):\n                tmp_frames, tmp_coor, tmp_vis = [], [], []\n                # Construct a single input sequence\n                for f in range(self.num_frame):\n                    if os.path.exists(frame_file[i+f]):\n                        tmp_frames.append(frame_file[i+f])\n                        tmp_coor.append((x[i+f], y[i+f]))\n                        tmp_vis.append(vis[i+f])\n                    else:\n                        break\n                    \n                if len(tmp_frames) == self.num_frame:\n                    assert len(tmp_frames) == len(tmp_coor) == len(tmp_vis)\n                    frame_files = np.concatenate((frame_files, [tmp_frames]), axis=0)\n                    coordinates = np.concatenate((coordinates, [tmp_coor]), axis=0)\n                    visibility = np.concatenate((visibility, [tmp_vis]), axis=0)\n        \n        np.savez(os.path.join(\"/kaggle/working/\", f'f{self.num_frame}_s{self.slideing_step}_{self.split}.npz'), filename=frame_files, coordinates=coordinates, visibility=visibility)\n        # print(rally_dirs)\n        \n    def _gen_frame_unit(self, frame_dir):\n        frame_files = np.array([]).reshape(0, self.num_frame)\n        coordinates = np.array([], dtype=np.float32).reshape(0, self.num_frame, 2)\n        visibility = np.array([], dtype=np.float32).reshape(0, self.num_frame)\n        \n        match_dir, rally_id = parse.parse('{}/frame/{}', frame_dir)\n        csv_file = f'{match_dir}/csv/{rally_id}_ball.csv'\n        label_df = pd.read_csv(csv_file, encoding='utf8').sort_values(by='Frame')\n        frame_file = np.array([f'{frame_dir}/{f_id}.jpg' for f_id in label_df['Frame']])\n        x, y, vis = np.array(label_df['X']), np.array(label_df['Y']), np.array(label_df['Visibility'])\n        assert len(frame_file) == len(x) == len(y) == len(vis)\n\n        # Sliding on the frame sequence\n        for i in range(0, len(frame_file)-self.num_frame, self.slideing_step):\n            tmp_frames, tmp_coor, tmp_vis = [], [], []\n            # Construct a single input sequence\n            for f in range(self.num_frame):\n                if os.path.exists(frame_file[i+f]):\n                    tmp_frames.append(frame_file[i+f])\n                    tmp_coor.append((x[i+f], y[i+f]))\n                    tmp_vis.append(vis[i+f])\n\n            # Append the input sequence\n            if len(tmp_frames) == self.num_frame:\n                assert len(tmp_frames) == len(tmp_coor) == len(tmp_vis)\n                frame_files = np.concatenate((frame_files, [tmp_frames]), axis=0)\n                coordinates = np.concatenate((coordinates, [tmp_coor]), axis=0)\n                visibility = np.concatenate((visibility, [tmp_vis]), axis=0)\n\n        return frame_files, coordinates, visibility\n\n    def _get_heatmap(self, cx, cy, visible):\n        if not visible:\n            return np.zeros((1, self.HEIGHT, self.WIDTH)) if self.mode == '2d' else np.zeros((1, 1, self.HEIGHT, self.WIDTH))\n        x, y = np.meshgrid(np.linspace(1, self.WIDTH, self.WIDTH), np.linspace(1, self.HEIGHT, self.HEIGHT))\n        heatmap = ((y - (cy + 1))**2) + ((x - (cx + 1))**2)\n        heatmap[heatmap <= self.sigma**2] = 1.\n        heatmap[heatmap > self.sigma**2] = 0.\n        heatmap = heatmap * self.mag\n        return heatmap.reshape(1, self.HEIGHT, self.WIDTH) if self.mode == '2d' else heatmap.reshape(1, 1, self.HEIGHT, self.WIDTH)\n\n    def __len__(self):\n        return len(self.frame_files)\n\n    def __getitem__(self, idx):\n        frame_file = self.frame_files[idx]\n        coors = self.coordinates[idx]\n        vis = self.visibility[idx]\n\n        # Get the resize scaler\n        h, w, _ = cv2.imread(frame_file[0]).shape\n        h_ratio, w_ratio = h / self.HEIGHT, w / self.WIDTH\n\n        # Transform the coordinate\n        coors[:, 0] = coors[:, 0] / h_ratio\n        coors[:, 1] = coors[:, 1] / w_ratio\n\n        if self.mode == '2d':\n            frames = np.array([]).reshape(0, self.HEIGHT, self.WIDTH)\n            heatmaps = np.array([]).reshape(0, self.HEIGHT, self.WIDTH)\n\n            for i in range(self.num_frame):\n                img = tf.keras.utils.load_img(frame_file[i])\n                img = tf.keras.utils.img_to_array(img.resize(size=(self.WIDTH, self.HEIGHT)))\n                img = np.moveaxis(img, -1, 0)\n                frames = np.concatenate((frames, img), axis=0)\n                heatmap = self._get_heatmap(int(coors[i][0]), int(coors[i][1]), vis[i])\n                heatmaps = np.concatenate((heatmaps, heatmap), axis=0)        \n        else:\n            frames = np.array([]).reshape(3, 0, self.HEIGHT, self.WIDTH)\n            heatmaps = np.array([]).reshape(1, 0, self.HEIGHT, self.WIDTH)\n\n            for i in range(self.num_frame):\n                img = tf.keras.utils.load_img(frame_file[i])\n                img = tf.keras.utils.img_to_array(img.resize(size=(self.WIDTH, self.HEIGHT)))\n                img = np.moveaxis(img, -1, 0) \n                img = img.reshape(3, 1, self.HEIGHT, self.WIDTH)\n                frames = np.concatenate((frames, img), axis=1)\n                heatmap = self._get_heatmap(int(coors[i][0]), int(coors[i][1]), vis[i])\n                heatmaps = np.concatenate((heatmaps, heatmap), axis=1)\n        \n        frames /= 255.\n        return idx, frames, heatmaps, coors ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:36:06.398049Z","iopub.execute_input":"2024-06-03T04:36:06.398431Z","iopub.status.idle":"2024-06-03T04:36:06.441157Z","shell.execute_reply.started":"2024-06-03T04:36:06.398400Z","shell.execute_reply":"2024-06-03T04:36:06.440174Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport time\nimport argparse\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm\n\ntorch.backends.cudnn.benchmark = True \n\nmodel_name = \"TrackNetV2\"\nnum_frame = 3\ninput_type = \"2d\"\nepochs = 15\nbatch_size = 8\nlearning_rate = 0.001\ntolerance = 4\nsave_dir = \"exp\"\nresume_training = False\ndebug = False\nsave_dir = f'{save_dir}_debug' if debug else save_dir\ndisplay_step = 4 if debug else 100\n\n\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\n# Load dataset\nprint(f'Data dir: {data_dir}')\nprint(f'Data input type: {input_type}')\ntrain_dataset = Badminton_Dataset(root_dir=data_dir, split='train', mode=input_type, num_frame=num_frame, slideing_step=1, debug=debug)\neval_test_dataset = Badminton_Dataset(root_dir=data_dir, split='test', mode=input_type, num_frame=num_frame, slideing_step=num_frame, debug=debug)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True, pin_memory=False) #已更改pin_memory=False\neval_loader = DataLoader(eval_test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=False, pin_memory=False) #已更改pin_memory=False\nif __name__ == '__main__':\n\n    # create model\n    model = get_model(model_name, num_frame, input_type).cuda()\n    model_summary(model, model_name)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    if not resume_training:\n        loss_list = []\n        test_acc_dict = {'TP':[], 'TN': [], 'FP1': [], 'FP2': [], 'FN': [], 'accuracy': [], 'precision': [], 'recall': []}\n        start_epoch = 0\n        max_test_acc = 0.\n    else:\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        loss_list = checkpoint['loss_list']\n        test_acc_dict = checkpoint['test_acc']\n        start_epoch = checkpoint['epoch'] + 1\n        max_test_acc = np.max(test_acc_dict['accuracy'])\n        print(f'Resume training from epoch {start_epoch}.')\n\n    # training loop\n    train_start_time = time.time()\n    for epoch in range(start_epoch, epochs):\n        start_time = time.time()\n        loss = train(epoch, model, optimizer, WeightedBinaryCrossEntropy, train_loader, input_type, display_step, save_dir)\n        loss_list.append(loss)\n        torch.save(dict(epoch=epoch,\n                        model_state_dict=model.state_dict(),\n                        optimizer_state_dict=optimizer.state_dict(),\n#                         param_dict=param_dict,\n                        loss_list=loss_list,\n                        test_acc=test_acc_dict), f'{save_dir}/model_cur.pt')\n\n        accuracy, precision, recall, TP, TN, FP1, FP2, FN = evaluation(model, eval_loader, tolerance, input_type)\n        TP, TN, FP1, FP2, FN = len(TP), len(TN), len(FP1), len(FP2), len(FN)\n        #print(f'\\nacc: {accuracy:.4f}\\tprecision: {precision:.4f}\\trecall: {recall:.4f}\\tTP: {TP}\\tTN: {TN}\\tFP1: {FP1}\\tFP2: {FP2}\\tFN: {FN}')\n        \n        test_acc_dict['TP'].append(TP)\n        test_acc_dict['TN'].append(TN)\n        test_acc_dict['FP1'].append(FP1)\n        test_acc_dict['FP2'].append(FP2)\n        test_acc_dict['FN'].append(FN)\n        test_acc_dict['accuracy'].append(accuracy)\n        test_acc_dict['precision'].append(precision)\n        test_acc_dict['recall'].append(recall)\n\n        print(f'[epoch: {epoch})]\\tEpoch runtime: {(time.time() - start_time) / 3600.:.2f} hrs')\n        plot_result(loss_list, None, test_acc_dict, num_frame, save_dir, model_name)\n        \n        if test_acc_dict['accuracy'][-1] >= max_test_acc:\n            max_test_acc = test_acc_dict['accuracy'][-1]\n            torch.save(dict(epoch=epoch,\n                            model_state_dict=model.state_dict(),\n                            optimizer_state_dict=optimizer.state_dict(),\n#                             param_dict=param_dict,\n                            loss_list=loss_list,\n                            test_acc=test_acc_dict), f'{save_dir}/model_best.pt')\n\n    torch.save(dict(epoch=epoch,\n                    model_state_dict=model.state_dict(),\n                    optimizer_state_dict=optimizer.state_dict(),\n#                     param_dict=param_dict,\n                    loss_list=loss_list,\n                    test_acc=test_acc_dict), f'{save_dir}/model_last.pt')\n\n    print(f'runtime: {(time.time() - train_start_time) / 3600.:.2f} hrs')\n    print('Done......')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction on a video using best_model","metadata":{}},{"cell_type":"code","source":"video_file = \"/kaggle/input/tracknet-v3-model-testing/00002.mp4\"\nmodel_file = \"/kaggle/input/model-for-ball/exp/model_best.pt\"\nnum_frame = 3\nbatch_size = 1\nsave_dir = \"/kaggle/working/\"\n\nvideo_name = video_file.split('/')[-1][:-4]\nvideo_format = video_file.split('/')[-1][-3:]\nout_video_file = f'{save_dir}/{video_name}_pred.{video_format}'\nout_csv_file = f'{save_dir}/{video_name}_ball.csv'\n\ncheckpoint = torch.load(model_file)\n# param_dict = checkpoint['param_dict']\nmodel_name = \"TrackNetV2\"\nnum_frame = 3\ninput_type = \"2d\"\n\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n \n# Load model\nmodel = get_model(model_name, num_frame, input_type).cuda()\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Video output configuration\nif video_format == 'avi':\n    fourcc = cv2.VideoWriter_fourcc(*'DIVX')\nelif video_format == 'mp4':\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\nelse:\n    raise ValueError('Invalid video format.')\n\n# Write csv file head\nf = open(out_csv_file, 'w')\nf.write('Frame,Visibility,X,Y\\n')\n\n# Cap configuration\ncap = cv2.VideoCapture(video_file)\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nw = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nh = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nsuccess = True\nframe_count = 0\nnum_final_frame = 0\nratio = h / HEIGHT\nout = cv2.VideoWriter(out_video_file, fourcc, fps, (w, h))\n\nwhile success:\n    print(f'Number of sampled frames: {frame_count}')\n    # Sample frames to form input sequence\n    frame_queue = []\n    for _ in range(num_frame*batch_size):\n        success, frame = cap.read()\n        if not success:\n            break\n        else:\n            frame_count += 1\n            frame_queue.append(frame)\n\n    if not frame_queue:\n        break\n    \n    # If mini batch incomplete\n    if len(frame_queue) % num_frame != 0:\n        frame_queue = []\n        # Record the length of remain frames\n        num_final_frame = len(frame_queue) +1\n        print(num_final_frame)\n        # Adjust the sample timestampe of cap\n        frame_count = frame_count - num_frame*batch_size\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n        # Re-sample mini batch\n        for _ in range(num_frame*batch_size):\n            success, frame = cap.read()\n            if not success:\n                break\n            else:\n                frame_count += 1\n                frame_queue.append(frame)\n        if len(frame_queue) % num_frame != 0:\n            continue\n    \n    x = get_frame_unit(frame_queue, num_frame)\n    \n    # Inference\n    with torch.no_grad():\n        y_pred = model(x.cuda())\n    y_pred = y_pred.detach().cpu().numpy()\n    h_pred = y_pred > 0.5\n    h_pred = h_pred * 255.\n    h_pred = h_pred.astype('uint8')\n    h_pred = h_pred.reshape(-1, HEIGHT, WIDTH)\n    \n    \n    def find_rectangle_from_intersection(intersection_x, intersection_y, height=22, width=22):\n        left = intersection_x - width / 2\n        top = intersection_y - height / 2\n        return float(top), float(left), float(height), float(width)\n\n    for i in range(h_pred.shape[0]):\n        if num_final_frame > 0 and i < (num_frame*batch_size - num_final_frame-1):\n            print('aaa')\n            # Special case of last incomplete mini batch\n            # Igore the frame which is already written to the output video\n            continue \n        else:\n            img = frame_queue[i].copy()\n            cx_pred, cy_pred = get_object_center(h_pred[i])\n            cx_pred, cy_pred = int(ratio*cx_pred), int(ratio*cy_pred)\n            vis = 1 if cx_pred > 0 and cy_pred > 0 else 0\n            # Write prediction result\n            f.write(f'{frame_count-(num_frame*batch_size)+i},{vis},{cx_pred},{cy_pred}\\n')\n            # print(frame_count-(num_frame*batch_size)+i)\n            if cx_pred != 0 or cy_pred != 0:\n                top, left, height, width = find_rectangle_from_intersection(cx_pred, cy_pred)\n                cv2.rectangle(img, (int(left), int(top)), (int(left + width), int(top + height)), (255, 0, 0), 2)\n#                 cv2.circle(img, (cx_pred, cy_pred), 5, (0, 0, 255), -1)\n            out.write(img)\n\nout.release()\nprint('Done.')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:37:38.761486Z","iopub.execute_input":"2024-06-03T04:37:38.762155Z","iopub.status.idle":"2024-06-03T04:38:18.682573Z","shell.execute_reply.started":"2024-06-03T04:37:38.762125Z","shell.execute_reply":"2024-06-03T04:38:18.681606Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Number of sampled frames: 0\nNumber of sampled frames: 3\nNumber of sampled frames: 6\nNumber of sampled frames: 9\nNumber of sampled frames: 12\nNumber of sampled frames: 15\nNumber of sampled frames: 18\nNumber of sampled frames: 21\nNumber of sampled frames: 24\nNumber of sampled frames: 27\nNumber of sampled frames: 30\nNumber of sampled frames: 33\nNumber of sampled frames: 36\nNumber of sampled frames: 39\nNumber of sampled frames: 42\nNumber of sampled frames: 45\nNumber of sampled frames: 48\nNumber of sampled frames: 51\nNumber of sampled frames: 54\nNumber of sampled frames: 57\nNumber of sampled frames: 60\nNumber of sampled frames: 63\nNumber of sampled frames: 66\nNumber of sampled frames: 69\nNumber of sampled frames: 72\nNumber of sampled frames: 75\nNumber of sampled frames: 78\nNumber of sampled frames: 81\nNumber of sampled frames: 84\nNumber of sampled frames: 87\nNumber of sampled frames: 90\nNumber of sampled frames: 93\nNumber of sampled frames: 96\nNumber of sampled frames: 99\nNumber of sampled frames: 102\nNumber of sampled frames: 105\nNumber of sampled frames: 108\nNumber of sampled frames: 111\nNumber of sampled frames: 114\nNumber of sampled frames: 117\nNumber of sampled frames: 120\nNumber of sampled frames: 123\nNumber of sampled frames: 126\nNumber of sampled frames: 129\nNumber of sampled frames: 132\nNumber of sampled frames: 135\nNumber of sampled frames: 138\nNumber of sampled frames: 141\nNumber of sampled frames: 144\nNumber of sampled frames: 147\nNumber of sampled frames: 150\nNumber of sampled frames: 153\nNumber of sampled frames: 156\nNumber of sampled frames: 159\nNumber of sampled frames: 162\nNumber of sampled frames: 165\nNumber of sampled frames: 168\nNumber of sampled frames: 171\nNumber of sampled frames: 174\nNumber of sampled frames: 177\nNumber of sampled frames: 180\nNumber of sampled frames: 183\nNumber of sampled frames: 186\nNumber of sampled frames: 189\nNumber of sampled frames: 192\nNumber of sampled frames: 195\nNumber of sampled frames: 198\nNumber of sampled frames: 201\nNumber of sampled frames: 204\nNumber of sampled frames: 207\nNumber of sampled frames: 210\nNumber of sampled frames: 213\nNumber of sampled frames: 216\nNumber of sampled frames: 219\nNumber of sampled frames: 222\nNumber of sampled frames: 225\nNumber of sampled frames: 228\nNumber of sampled frames: 231\nNumber of sampled frames: 234\nNumber of sampled frames: 237\nNumber of sampled frames: 240\nNumber of sampled frames: 243\nNumber of sampled frames: 246\nNumber of sampled frames: 249\nNumber of sampled frames: 252\nNumber of sampled frames: 255\nNumber of sampled frames: 258\nNumber of sampled frames: 261\nNumber of sampled frames: 264\nNumber of sampled frames: 267\nNumber of sampled frames: 270\nNumber of sampled frames: 273\nNumber of sampled frames: 276\nNumber of sampled frames: 279\nNumber of sampled frames: 282\nNumber of sampled frames: 285\nNumber of sampled frames: 288\nNumber of sampled frames: 291\nNumber of sampled frames: 294\nNumber of sampled frames: 297\nNumber of sampled frames: 300\nNumber of sampled frames: 303\nNumber of sampled frames: 306\nNumber of sampled frames: 309\nNumber of sampled frames: 312\nNumber of sampled frames: 315\nNumber of sampled frames: 318\nNumber of sampled frames: 321\nNumber of sampled frames: 324\nNumber of sampled frames: 327\nNumber of sampled frames: 330\nNumber of sampled frames: 333\nNumber of sampled frames: 336\nNumber of sampled frames: 339\nNumber of sampled frames: 342\nNumber of sampled frames: 345\nNumber of sampled frames: 348\nNumber of sampled frames: 351\nNumber of sampled frames: 354\nNumber of sampled frames: 357\nNumber of sampled frames: 360\nNumber of sampled frames: 363\nNumber of sampled frames: 366\nNumber of sampled frames: 369\nNumber of sampled frames: 372\nNumber of sampled frames: 375\nNumber of sampled frames: 378\nNumber of sampled frames: 381\nNumber of sampled frames: 384\nNumber of sampled frames: 387\nNumber of sampled frames: 390\nNumber of sampled frames: 393\nNumber of sampled frames: 396\nNumber of sampled frames: 399\nNumber of sampled frames: 402\nNumber of sampled frames: 405\nNumber of sampled frames: 408\nNumber of sampled frames: 411\nNumber of sampled frames: 414\nNumber of sampled frames: 417\nNumber of sampled frames: 420\nNumber of sampled frames: 423\nNumber of sampled frames: 426\nNumber of sampled frames: 429\nNumber of sampled frames: 432\nNumber of sampled frames: 435\nNumber of sampled frames: 438\nNumber of sampled frames: 441\nNumber of sampled frames: 444\nNumber of sampled frames: 447\nNumber of sampled frames: 450\nNumber of sampled frames: 453\nNumber of sampled frames: 456\nNumber of sampled frames: 459\nNumber of sampled frames: 462\nNumber of sampled frames: 465\nNumber of sampled frames: 468\nNumber of sampled frames: 471\nNumber of sampled frames: 474\nNumber of sampled frames: 477\nNumber of sampled frames: 480\nNumber of sampled frames: 483\nNumber of sampled frames: 486\nNumber of sampled frames: 489\nNumber of sampled frames: 492\nNumber of sampled frames: 495\nNumber of sampled frames: 498\nNumber of sampled frames: 501\nNumber of sampled frames: 504\nNumber of sampled frames: 507\nNumber of sampled frames: 510\nNumber of sampled frames: 513\nNumber of sampled frames: 516\nNumber of sampled frames: 519\nNumber of sampled frames: 522\nNumber of sampled frames: 525\nNumber of sampled frames: 528\nNumber of sampled frames: 531\nNumber of sampled frames: 534\nNumber of sampled frames: 537\nNumber of sampled frames: 540\nNumber of sampled frames: 543\nNumber of sampled frames: 546\nNumber of sampled frames: 549\nNumber of sampled frames: 552\nNumber of sampled frames: 555\nNumber of sampled frames: 558\nNumber of sampled frames: 561\nNumber of sampled frames: 564\nNumber of sampled frames: 567\nNumber of sampled frames: 570\nNumber of sampled frames: 573\nNumber of sampled frames: 576\nNumber of sampled frames: 579\nNumber of sampled frames: 582\nNumber of sampled frames: 585\nNumber of sampled frames: 588\nNumber of sampled frames: 591\nNumber of sampled frames: 594\nNumber of sampled frames: 597\nNumber of sampled frames: 600\nNumber of sampled frames: 603\nNumber of sampled frames: 606\nNumber of sampled frames: 609\nNumber of sampled frames: 612\nNumber of sampled frames: 615\nNumber of sampled frames: 618\nNumber of sampled frames: 621\nNumber of sampled frames: 624\nNumber of sampled frames: 627\nNumber of sampled frames: 630\nNumber of sampled frames: 633\nNumber of sampled frames: 636\nNumber of sampled frames: 639\nNumber of sampled frames: 642\nNumber of sampled frames: 645\nNumber of sampled frames: 648\nNumber of sampled frames: 651\nNumber of sampled frames: 654\nNumber of sampled frames: 657\nNumber of sampled frames: 660\nNumber of sampled frames: 663\nNumber of sampled frames: 666\nNumber of sampled frames: 669\nNumber of sampled frames: 672\nNumber of sampled frames: 675\nNumber of sampled frames: 678\nNumber of sampled frames: 681\nNumber of sampled frames: 684\nNumber of sampled frames: 687\nNumber of sampled frames: 690\nNumber of sampled frames: 693\nNumber of sampled frames: 696\nNumber of sampled frames: 699\nNumber of sampled frames: 702\nNumber of sampled frames: 705\nNumber of sampled frames: 708\nNumber of sampled frames: 711\nNumber of sampled frames: 714\nNumber of sampled frames: 717\nNumber of sampled frames: 720\nNumber of sampled frames: 723\nNumber of sampled frames: 726\nNumber of sampled frames: 729\nNumber of sampled frames: 732\nNumber of sampled frames: 735\nNumber of sampled frames: 738\nNumber of sampled frames: 741\nNumber of sampled frames: 744\nNumber of sampled frames: 747\nNumber of sampled frames: 750\nDone.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}